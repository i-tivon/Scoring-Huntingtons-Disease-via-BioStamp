function [trainedClassifiers, modelList, validationAccuracies] = trainClassifiers(inputTable)
% [trainedClassifiers, validationAccuracies] = trainClassifiers(inputTable)
% returns a trained classifier and its accuracy. This code recreates the
% classification model trained in Classification Learner app. Use the
% generated code to automate training the same model with new data, or to
% learn how to programmatically train models.
%
%  Input:
%      trainingData: a table containing the same predictor and response
%       columns as imported into the app.
%
%  Output:
%      trainedClassifier: a struct containing the trained classifiers. The
%       struct contains various fields with information about the trained
%       classifiers.
%      validationAccuracies: a struct containing the validation accuracies
%      in percent for each model trained


% Extract predictors and response
% This code processes the data into the right shape for training the
% model.

% Modified code that was auto-generated by MATLAB on 18-Jan-2020 12:14:53

predictorNames = inputTable.Properties.VariableNames(1:end-1);
predictors = inputTable(:, predictorNames);
response = inputTable.predictor;

predictorExtractionFcn = @(t) t(:, predictorNames);
modelList=cell(1,9);
ctr=1; 

% Train classifiers
%% (1) Linear Descriminant

classificationDiscriminant = fitcdiscr(...
    predictors, response, 'DiscrimType', 'linear', 'Gamma', 0, ...
    'FillCoeffs', 'off', 'ClassNames', [0; 1]);

% Create the result struct with predict function
discriminantPredictFcn = @(x) predict(classificationDiscriminant, x);
trainedClassifiers{ctr}.predictFcn = @(x) discriminantPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationDiscriminant = classificationDiscriminant;
trainedClassifiers{ctr}.model_name = 'linear discriminant';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

%  Compute validation accuracy
partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationDiscriminant, 'KFold', 5)
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;
%% (2) Logistic Regression

% 1 or true = 'successful' class
% 0 or false = 'failure' class
successClass = double(1);
failureClass = double(0);

% Compute the majority response class. If there is a NaN-prediction from
% fitglm, convert NaN to this majority class label.
numSuccess = sum(response == successClass);
numFailure = sum(response == failureClass);
if numSuccess > numFailure
    missingClass = successClass;
else
    missingClass = failureClass;
end
successFailureAndMissingClasses = [successClass; failureClass; missingClass];
isMissing = isnan(response);
zeroOneResponse = double(ismember(response, successClass));
zeroOneResponse(isMissing) = NaN;
% Prepare input arguments to fitglm.
concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
% Train using fitglm.
GeneralizedLinearModel = fitglm(concatenatedPredictorsAndResponse,...
    'Distribution', 'binomial','link', 'logit');

% Convert predicted probabilities to predicted class labels and scores.
convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
scoresFcn = @(p) [1-p, p];
predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );

trainedClassifiers{ctr}.predictFcn = @(x) logisticRegressionPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.GeneralizedLinearModel = GeneralizedLinearModel;
trainedClassifiers{ctr}.SuccessClass = successClass;
trainedClassifiers{ctr}.FailureClass = failureClass;
trainedClassifiers{ctr}.MissingClass = missingClass;
trainedClassifiers{ctr}.ClassNames = {successClass; failureClass};
trainedClassifiers{ctr}.model_name = 'logistic regression';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

KFolds=5;
cvp = cvpartition(response, 'KFold', KFolds);

% Initialize the predictions to the proper sizes
validationPredictions = response;
numObservations = size(predictors, 1);
numClasses = 2;
validationScores = NaN(numObservations, numClasses);
for fold = 1:KFolds
    trainingPredictors = predictors(cvp.training(fold), :);
    trainingResponse = response(cvp.training(fold), :);
    
    % Compute the majority response class. If there is a NaN-prediction from
    % fitglm, convert NaN to this majority class label.
    numSuccess = sum(trainingResponse == successClass);
    numFailure = sum(trainingResponse == failureClass);
    if numSuccess > numFailure
        missingClass = successClass;
    else
        missingClass = failureClass;
    end
    successFailureAndMissingClasses = [successClass; failureClass; missingClass];
    isMissing = isnan(trainingResponse);
    zeroOneResponse = double(ismember(trainingResponse, successClass));
    zeroOneResponse(isMissing) = NaN;
    concatenatedPredictorsAndResponse = [trainingPredictors, table(zeroOneResponse)];
    GeneralizedLinearModel = fitglm(concatenatedPredictorsAndResponse, ...
        'Distribution', 'binomial','link', 'logit');
    
    % Convert predicted probabilities to predicted class labels and scores.
    convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
    returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
    scoresFcn = @(p) [1-p, p];
    predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
    
    % Create the result struct with predict function
    logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
    validationPredictFcn = @(x) logisticRegressionPredictFcn(x);
    
    % Compute validation predictions
    validationPredictors = predictors(cvp.test(fold), :);
    [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);
    
    % Store predictions in the original order
    validationPredictions(cvp.test(fold), :) = foldPredictions;
    validationScores(cvp.test(fold), :) = foldScores;
end

% Compute validation accuracy
correctPredictions = (validationPredictions == response);
isMissing = isnan(response);
correctPredictions = correctPredictions(~isMissing);
validationAccuracies(ctr) = sum(correctPredictions)/length(correctPredictions);

ctr=ctr+1;
%% (3) linear SVM

classificationSVM = fitcsvm(...
    predictors, response,'KernelFunction', 'linear', 'PolynomialOrder', [], ...
    'KernelScale', 'auto','BoxConstraint', 1,'Standardize', true, 'ClassNames', [0; 1]);

% Create the result struct with predict function
svmPredictFcn = @(x) predict(classificationSVM, x);
trainedClassifiers{ctr}.predictFcn = @(x) svmPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationSVM = classificationSVM;
trainedClassifiers{ctr}.model_name = 'linear SVM';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationSVM, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;

%% (4) quadratic SVM

classificationSVM = fitcsvm(...
    predictors,response,'KernelFunction', 'polynomial', ...
    'PolynomialOrder', 2,'KernelScale', 'auto', ...
    'BoxConstraint', 1, 'Standardize', true,'ClassNames', [0; 1]);

% Create the result struct with predict function
svmPredictFcn = @(x) predict(classificationSVM, x);
trainedClassifiers{ctr}.predictFcn = @(x) svmPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationSVM = classificationSVM;
trainedClassifiers{ctr}.model_name = 'quadratic SVM';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationSVM, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;

%% (5) Medium gaussian SVM

classificationSVM = fitcsvm(predictors, response,'KernelFunction', 'gaussian', ...
    'PolynomialOrder', [], 'KernelScale', 3.5, 'BoxConstraint', 1, ...
    'Standardize', true, 'ClassNames', [0; 1]);

% Create the result struct with predict function
svmPredictFcn = @(x) predict(classificationSVM, x);
trainedClassifiers{ctr}.predictFcn = @(x) svmPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationSVM = classificationSVM;
trainedClassifiers{ctr}.model_name = 'medium gaussian SVM';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationSVM, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;

%% (6) fine KNN

classificationKNN = fitcknn(predictors, response, 'Distance', 'Euclidean',...
    'Exponent', [],'NumNeighbors', 1,'DistanceWeight', 'Equal','Standardize', true, ...
    'ClassNames', [0; 1]);

% Create the result struct with predict function
knnPredictFcn = @(x) predict(classificationKNN, x);
trainedClassifiers{ctr}.predictFcn = @(x) knnPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationKNN = classificationKNN;
trainedClassifiers{ctr}.model_name = 'fine KNN';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationKNN, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;

%% (7) medium KNN
classificationKNN = fitcknn(predictors, response, 'Distance', 'Euclidean',...
    'Exponent', [],'NumNeighbors', 10,'DistanceWeight', 'Equal','Standardize', true, ...
    'ClassNames', [0; 1]);

% Create the result struct with predict function
knnPredictFcn = @(x) predict(classificationKNN, x);
trainedClassifiers{ctr}.predictFcn = @(x) knnPredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationKNN = classificationKNN;
trainedClassifiers{ctr}.model_name = 'medium KNN';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationKNN, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;

%% (8) ensemble subspace discriminant 

subspaceDimension = max(1, min(6, width(predictors) - 1));
classificationEnsemble = fitcensemble(predictors,response,'Method', 'Subspace', ...
    'NumLearningCycles', 30,'Learners', 'discriminant','NPredToSample', subspaceDimension, ...
    'ClassNames', [0; 1]);

% Create the result struct with predict function
ensemblePredictFcn = @(x) predict(classificationEnsemble, x);
trainedClassifiers{ctr}.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationEnsemble = classificationEnsemble;
trainedClassifiers{ctr}.model_name = 'ensemble subspace discriminant';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationEnsemble, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

ctr=ctr+1;
%% (9) ensemble subspace KNN

subspaceDimension = max(1, min(6, width(predictors) - 1));
classificationEnsemble = fitcensemble(predictors,response,'Method', 'Subspace', ...
    'NumLearningCycles', 30,'Learners', 'knn','NPredToSample', subspaceDimension, ...
    'ClassNames', [0; 1]);

% Create the result struct with predict function
ensemblePredictFcn = @(x) predict(classificationEnsemble, x);
trainedClassifiers{ctr}.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));
trainedClassifiers{ctr}.RequiredVariables = predictorNames;
trainedClassifiers{ctr}.ClassificationEnsemble = classificationEnsemble;
trainedClassifiers{ctr}.model_name = 'ensemble subspace KNN';
modelList{ctr}=trainedClassifiers{ctr}.model_name;

partitionedModel = crossval(trainedClassifiers{ctr}.ClassificationEnsemble, 'KFold', 5);
validationAccuracies(ctr) = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

